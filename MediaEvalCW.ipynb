{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "import pandas\n",
    "import numpy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from langdetect import detect\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching dataset\n",
    "url = \"https://github.com/signerebassoo/COMP3222/blob/master/assignment-comp3222-comp6246-mediaeval2015-dataset.zip?raw=true\"\n",
    "filename = \"mediaeval.zip\"\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "#Extracting zip\n",
    "zfile = zipfile.ZipFile(\"mediaeval.zip\", \"r\")\n",
    "zfile.extractall()\n",
    "zfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pandas.read_csv(\"mediaeval-2015-trainingset.txt\", sep=\"\t\")\n",
    "testData = pandas.read_csv(\"mediaeval-2015-testset.txt\", sep=\"\t\")\n",
    "\n",
    "#Creating DataFrames for training and testing\n",
    "df_train = pandas.DataFrame(data = trainData)\n",
    "df_test = pandas.DataFrame(data = testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Characterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId                                          tweetText  \\\n",
       "0  263046056240115712  ¿Se acuerdan de la película: “El día después d...   \n",
       "1  262995061304852481  @milenagimon: Miren a Sandy en NY!  Tremenda i...   \n",
       "2  262979898002534400  Buena la foto del Huracán Sandy, me recuerda a...   \n",
       "3  262996108400271360     Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4  263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 7 columns):\n",
      "tweetId       14277 non-null int64\n",
      "tweetText     14277 non-null object\n",
      "userId        14277 non-null int64\n",
      "imageId(s)    14277 non-null object\n",
      "username      14277 non-null object\n",
      "timestamp     14277 non-null object\n",
      "label         14277 non-null object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 780.9+ KB\n"
     ]
    }
   ],
   "source": [
    "trainData.info() #Metadata of training data, including size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3755, 7)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape #Size of testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs\n",
      "boston                 546\n",
      "bringback              131\n",
      "columbianChemicals     185\n",
      "elephant                13\n",
      "livr                     9\n",
      "malaysia               501\n",
      "passport                46\n",
      "pigFish                 14\n",
      "sandyA                9695\n",
      "sandyB                2621\n",
      "sochi                  402\n",
      "underwater             112\n",
      "Name: tweetId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Determine events covered and their frequency by image names in training data\n",
    "df_train.rename(columns = {'imageId(s)':'imgs'}, inplace = True)\n",
    "imgCount = df_train.groupby(df_train.imgs.str.split('_').str[0])['tweetId'].nunique()\n",
    "print (imgCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs\n",
      "eclipse        277\n",
      "garissa         77\n",
      "nepal         1353\n",
      "samurai        218\n",
      "syrianboy     1769\n",
      "varoufakis      61\n",
      "Name: tweetId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Determine events covered and their frequency by image names in testing data\n",
    "df_test.rename(columns = {'imageId(s)':'imgs'}, inplace = True)\n",
    "imgCount = df_test.groupby(df_test.imgs.str.split('_').str[0])['tweetId'].nunique()\n",
    "print (imgCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Se acuerdan de la película: “El día después de mañana”? Me recuerda a lo que está pasando con el huracán #Sandy.\n",
      "Buena la foto del Huracán Sandy, me recuerda a la película Día de la Independencia #ID4 #Sandy\n",
      "Scary shit #hurricane #NY\n",
      "My fave place in the world #nyc #hurricane #sandy #statueofliberty\n",
      "42nd #time #square #NYC #subway #hurricane\n",
      "Just in time for #halloween a photo of #hurricane #sandy #frankenstorm\n",
      "Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast\n",
      "#sandy #newyork #hurricane #statueofliberty #USA\n",
      "#nyc #hurricane\n",
      "robertosalibaba god be with u brother #sandy #hurricane #newyork\n",
      "#Crazy #Hurricane #Sandy\n",
      "#shark #newjersey #swim #sandy #hurricane\n",
      "Good luck #ny #newyork #usa #hurricane #sandy\n",
      "Wow.... Fishing anyone? #hurricane #sandy\n",
      "Well #howdy there #hurricane #sandy . Just wanted to let you know that you took my power, internet, happi\n",
      "Just known this bcs of #jason #chen updated the pic! Everyone be safe! #newyork #sandy #hurricane #nature #e\n",
      "My thoughts and prayers go to all of the people going thru #hurricane #sandy #ct #de #ma #md #me #nc #nh #nj\n",
      "Stay safe my New York family...#nyc #newyork #storm #hurricane #wind\n",
      "New York #hurricane #Sandy\n",
      "Probably the coolest pic of #hurricanesandy! #hurricane #sandy #weather #storm #statueofliberty\n",
      "Crazy #sandy #hurricane images. Be glad we live in the #west coast.\n",
      "My cousin sent this to me... :: Cleveland voice :: We gon' die!!! #hurricane #hurricanesandy #sandy\n",
      "We forget tomb unknown soldier is guard 24/7365 ! Taken today #hurricane #sandy #frankenstorm\n",
      "“ #new #york #hurricane #sandy Bruh\n",
      "New York \"attacked\" by Sandy. #NewYork #statueofliberty #hurricane #Sandy #dark #attack #sky #picoftheday #\n",
      "This is sooo cute! #puupy #dog #hurricane #sandy #cute #adorable #flood #trees #golden #retriever #classic\n",
      "empty streets #nyc #hurricane #sandy\n",
      "Shark. #Sandy #JerseyIThink #Hurricane #NYC #SomethingIsntRight #Animals #Shark #NaturalDisasterOrNot\n",
      "Craziest picture ever #hurricane\n",
      "#hurricane #sandy #WOW #NYC\n",
      "So New York is getting a little rain... #Hurricane #Sandy\n",
      "The Tomb of the Unknown #Marines never take the #DayOff #Respect #NoDaysOff #Hurricane #Sandy #Superstorm\n",
      "Lord have mercy on their souls #Hurricane #Sandy #NewYork #RIP\n",
      "Look at this amazing pic taken today of #sandy #NYC #NewYorkCity #LadyLiberty #Hurricane #Amazing #Weather\n",
      "Mans best friend #love #hurricane #sandy #dog\n",
      "#hurricane\n",
      "Terrifying. #NY #hurricane #sandy #statueofliberty\n",
      "#hurricane #sandy lookin like a bag of money\n",
      "Crazy shot of #Hurricane #Sandy\n",
      "UMMMM #shark #hurricane #nj WOAHH\n",
      "#amazing #photography #nature #hurricane #sandy #clouds\n",
      "So...that's a shark swimming in someone's front yard in NJ. #Sandy #shark #landshark #hurricane #fishing #be\n",
      "uh #hurricane #sandy #wind #blowing #cloudy #water #### #\n",
      "#hurricane #sandy #NY #NewYork\n",
      "#hurricane #sandy #NY #newyork #america #usa #sky #tube #instago #instaboy #instahub #i#instagood #instagram\n",
      "#NewYork #Crazy #Statue #Liberty #Hurricane #Today #Nature #2012\n",
      "#shark #nyc #streets #sandy #hurricane\n",
      "A picture someone took of a shark swimming by their house when it got flooded #NewJersey #Hurricane #Sand\n",
      "I hope the East Coast is doing well in those times. Thoughts are with y'all xo #hurricane #sandy #eastcoast\n",
      "#2012 #statueofliberty #NYC #world #ends #hurricane #sandy\n",
      "#NY #hurricane #cool\n",
      "The Eye of The Storm... #Hurricane #Sandy\n",
      "#timesquare #newyork #nyc #ghostcity #hurricane #sandy\n",
      "#newyork #hurricane #sandy\n",
      "Hope all my family and friends on the east coast stay safe!! #Hurricane #sandy #ilovecali #storm #clouds #ra\n",
      "#hurricane\n",
      "#Hurricane #Sandy #NYC\n",
      "WTF! LaGuardia Airport! #JetBlue #Travel #Sandy #Hurricane\n",
      "#newyork I'm prayin for y'all #hurricane #sand #realimage #jesus\n",
      "#Hurricane #Sandy #NYC\n",
      "Scary!!!! #newyork #hurricane #sandy #newyorkcity #ny #fuckingrun\n"
     ]
    }
   ],
   "source": [
    "#Helper to look into the tweetText of a particular event image to determine what the event is\n",
    "selector = []\n",
    "for imgs in df_train['imgs']:\n",
    "    if \"sandy\" in imgs:\n",
    "        selector.append(True)\n",
    "    else:\n",
    "        selector.append(False)\n",
    "        \n",
    "isEvent = pandas.Series(selector)\n",
    "\n",
    "df_event = df_train[isEvent].head(61)\n",
    "\n",
    "for tweet in df_event['tweetText']:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man sandy Foreal??  ⚡⚡⚡☔☔⚡🌊🌊☁🚣⛵💡🔌🚬🚬🚬🔫🔫🔒🔒🔐🔑🔒🚪🚪🚪🔨🔨🔨🏊🏊🏊🏊🎣🎣🎣😱😰😖😫😩😤💨💨💨💨💦💦💦💧💦💥💥💥👽💩🙌🙌🙌🙌🙌🏃🏃🏃🏃🏃👫👭💏👪👪👬👭💑🙇🌕🌕🌕🌎 http://t.co/vEWVXy10\n"
     ]
    }
   ],
   "source": [
    "langs = dict()\n",
    "\n",
    "for tweet in df_train['tweetText']:\n",
    "    try:\n",
    "        lan = detect(tweet)\n",
    "    except:\n",
    "        pass\n",
    "        lan = \"Unknown\"\n",
    "        print(tweet)\n",
    "    if lan in langs.keys():\n",
    "        langs[lan] = langs[lan] + 1\n",
    "    else:\n",
    "        langs[lan] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'es': 1293, 'en': 10955, 'sq': 7, 'ru': 60, 'it': 102, 'no': 36, 'fr': 218, 'mk': 2, 'ro': 7, 'nl': 89, 'bg': 8, 'pt': 159, 'de': 130, 'tl': 315, 'cy': 117, 'ja': 22, 'ar': 81, 'vi': 13, 'ca': 34, 'sk': 15, 'hu': 6, 'sv': 42, 'so': 119, 'fi': 15, 'pl': 42, 'id': 173, 'da': 26, 'af': 70, 'el': 5, 'lt': 4, 'he': 1, 'hr': 6, 'tr': 33, 'zh-cn': 10, 'fa': 3, 'sl': 7, 'sw': 13, 'et': 11, 'ko': 6, 'th': 17, 'cs': 2, 'Unknown': 1, 'hi': 1, 'lv': 1}\n"
     ]
    }
   ],
   "source": [
    "print (langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing 'humor' to 'fake'\n",
    "df_train.loc[(df_train.label == 'humor'),'label'] = 'fake'\n",
    "df_test.loc[(df_test.label == 'humor'),'label'] = 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\signe\\Anaconda3\\envs\\Comp3222Labs\\lib\\site-packages\\pandas\\core\\strings.py:1843: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11550, 7)"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing retweets, reposts, and modified tweets\n",
    "rtPattern1 = \"(RT|rt|MT|mt|RP|rp):? @\\w*:?\"\n",
    "rtPattern2 = \"(\\bRT\\b|\\brt\\b|\\bMT\\b|\\bmt\\b|\\bRP\\b|\\brp\\b)\"\n",
    "rtPattern3 = \"(@\\w*:)\"\n",
    "rtPattern4 = \"(#rt|#RT|#mt|#MT|#rp|#retweet|#Retweet|#modifiedtweet|#modifiedTweet|#ModifiedTweet|#repost|#Repost)\"\n",
    "rtPattern5 = \"(via @\\w*)\"\n",
    "\n",
    "retweets = df_train['tweetText'].str.contains(rtPattern1)\n",
    "df_train = df_train[~retweets]\n",
    "\n",
    "retweets = df_train['tweetText'].str.contains(rtPattern2)\n",
    "df_train = df_train[~retweets]\n",
    "\n",
    "retweets = df_train['tweetText'].str.contains(rtPattern3)\n",
    "df_train = df_train[~retweets]\n",
    "\n",
    "retweets = df_train['tweetText'].str.contains(rtPattern4)\n",
    "df_train = df_train[~retweets]\n",
    "\n",
    "retweets = df_train['tweetText'].str.contains(rtPattern5)\n",
    "df_train = df_train[~retweets]\n",
    "\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing remaining twitter handles @username\n",
    "df_train['tweetText'] = df_train['tweetText'].apply(lambda text: re.sub(r'@\\w*', \"\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing emojis\n",
    "emojis = re.compile(\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                    u\"\\U00002702-\\U000027B0\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "df_train['tweetText'] = df_train['tweetText'].apply(lambda text: emojis.sub(r'', text) if emojis.search(text) else text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning symbols - ampersand and newline\n",
    "df_train['tweetText'] = df_train['tweetText'].apply(lambda text: re.sub(r'&amp;|\\\\n', '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing urls\n",
    "df_train['tweetText'] = df_train['tweetText'].apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "df_train['tweetText'] = df_train['tweetText'].apply(lambda text: re.sub(r'\\\\\\/\\S+', '', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you remember the movie: \"The after tomorrow\"? It reminds me of what's happening with Hurricane #Sandy.\n",
      "Good photo of Hurricane Sandy, reminds me of the movie Independence Day # ID4 #Sandy\n",
      "UMMMM #shark #hurricane #nj WOAHH\n",
      "#sandy #hurricane #fun #usa\n",
      "Shark on the highway highway #hurricane #sandy\n",
      "I never imagined imagine this scene in real life ... #chocada #eusoualenda #iamlegend #ny #newyork #hurricane #\n",
      "Holy frankenstorm! #newyork #frankenstorm #hurricane #sandy #insane\n",
      "All quiet #hurricane\n",
      "#newyork #hurricane beautiful and scary\n",
      "#Crazy picture. #Hurricane\n",
      "Wow ... What violently ... Hope mn family there remains unharmed .. #hurricane #Sandy\n",
      "Shark on the highway highway #hurricane #sandy\n",
      "HURRICANE SANDY: Sharks are found in the streets of New Jersey due to the advance of the sea caused by Hurricane Look:\n",
      "#SANDY: Sharks are found in the streets of New Jersey due to the advance of the sea caused by Hurricane Sandy. look:\n",
      "Sharks are found in the streets of New Jersey due to the advance of the sea caused by Hurricane Sandy.\n",
      "Sharks are found in the streets of New Jersey due to the advance of the sea caused by Hurricane Sandy. MY GOD\n",
      "#SANDY: Sharks are found in the streets of New Jersey due to the advance of the sea caused by Hurricane Sandy. look:\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-06d80054fa72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlan\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'en'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#TODO replace cell value to translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Comp3222Labs\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# this code will be updated when the format is changed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Comp3222Labs\\lib\\site-packages\\googletrans\\client.py\u001b[0m in \u001b[0;36m_translate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Comp3222Labs\\lib\\site-packages\\googletrans\\utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[1;34m(original)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Comp3222Labs\\lib\\site-packages\\googletrans\\utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[1;34m(original)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Comp3222Labs\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Comp3222Labs\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Comp3222Labs\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "#NOT USED\n",
    "#Translating to English\n",
    "\n",
    "i = 0\n",
    "for tweet in df_train['tweetText']:\n",
    "    try:\n",
    "        lan = detect(tweet)\n",
    "    except:\n",
    "        continue\n",
    "    if lan != 'en':\n",
    "        tr = Translator()\n",
    "        trText = tr.translate(tweet).text\n",
    "        print(trText)\n",
    "        #TODO replace cell value to translation\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing whitespace\n",
    "df_train['tweetText'] = df_train['tweetText'].apply(lambda text: \" \".join(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise stopwords\n",
    "stopwords = nltk.corpus.stopwords.words()\n",
    "stopwords.extend([':', ';', '[', ']', '\"', \"'\", '(', ')', '.', '?', '#', '@', '...'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords\n",
    "df_train['filteredTweet'] = df_train['tweetText'].apply(lambda x: ' '.join([w for w in x.split() if w not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imgs</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>filteredTweet</th>\n",
       "      <th>lemmatisedTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¿Se acuerdan de la película: “El día después d...</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>¿Se acuerdan película: “El día después mañana”...</td>\n",
       "      <td>¿Se acuerdan película: “El día después mañana”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Huracán Sandy, me recuerda a...</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>Buena foto Huracán Sandy, recuerda película Dí...</td>\n",
       "      <td>Buena foto Huracán Sandy, recuerda película Dí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>My fave place world #nyc #hurricane #sandy #st...</td>\n",
       "      <td>My fave place world #nyc #hurricane #sandy #st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>263364439582060545</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane</td>\n",
       "      <td>163674788</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>classycg</td>\n",
       "      <td>Tue Oct 30 19:39:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>262927032705490944</td>\n",
       "      <td>Just in time for #halloween a photo of #hurric...</td>\n",
       "      <td>246153081</td>\n",
       "      <td>sandyA_fake_14</td>\n",
       "      <td>j_unit87</td>\n",
       "      <td>Mon Oct 29 14:41:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>Just time #halloween photo #hurricane #sandy #...</td>\n",
       "      <td>Just time #halloween photo #hurricane #sandy #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>263321078884077568</td>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out ...</td>\n",
       "      <td>199565482</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>MrBlakMagik</td>\n",
       "      <td>Tue Oct 30 16:46:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>Crazy #Hurricane #Sandy prayers go family frie...</td>\n",
       "      <td>Crazy #Hurricane #Sandy prayer go family frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>263111677485142017</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA</td>\n",
       "      <td>78475739</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>safi37</td>\n",
       "      <td>Tue Oct 30 02:54:46 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>262977091983785985</td>\n",
       "      <td>#nyc #hurricane</td>\n",
       "      <td>869777653</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>kingmichael03</td>\n",
       "      <td>Mon Oct 29 17:59:59 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>#nyc #hurricane</td>\n",
       "      <td>#nyc #hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>262989009930833920</td>\n",
       "      <td>robertosalibaba god be with u brother #sandy #...</td>\n",
       "      <td>359592461</td>\n",
       "      <td>sandyA_fake_08</td>\n",
       "      <td>Michael_Saliba</td>\n",
       "      <td>Mon Oct 29 18:47:20 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>robertosalibaba god brother #sandy #hurricane ...</td>\n",
       "      <td>robertosalibaba god brother #sandy #hurricane ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId                                          tweetText  \\\n",
       "0  263046056240115712  ¿Se acuerdan de la película: “El día después d...   \n",
       "1  262979898002534400  Buena la foto del Huracán Sandy, me recuerda a...   \n",
       "2  262996108400271360                          Scary shit #hurricane #NY   \n",
       "3  263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
       "4  263364439582060545         42nd #time #square #NYC #subway #hurricane   \n",
       "5  262927032705490944  Just in time for #halloween a photo of #hurric...   \n",
       "6  263321078884077568  Crazy pic of #Hurricane #Sandy prayers go out ...   \n",
       "7  263111677485142017   #sandy #newyork #hurricane #statueofliberty #USA   \n",
       "8  262977091983785985                                    #nyc #hurricane   \n",
       "9  262989009930833920  robertosalibaba god be with u brother #sandy #...   \n",
       "\n",
       "      userId            imgs        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "2  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "3  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "4  163674788  sandyA_fake_23        classycg  Tue Oct 30 19:39:10 +0000 2012   \n",
       "5  246153081  sandyA_fake_14        j_unit87  Mon Oct 29 14:41:04 +0000 2012   \n",
       "6  199565482  sandyA_fake_29     MrBlakMagik  Tue Oct 30 16:46:52 +0000 2012   \n",
       "7   78475739  sandyA_fake_15          safi37  Tue Oct 30 02:54:46 +0000 2012   \n",
       "8  869777653  sandyA_fake_29   kingmichael03  Mon Oct 29 17:59:59 +0000 2012   \n",
       "9  359592461  sandyA_fake_08  Michael_Saliba  Mon Oct 29 18:47:20 +0000 2012   \n",
       "\n",
       "  label                                      filteredTweet  \\\n",
       "0  fake  ¿Se acuerdan película: “El día después mañana”...   \n",
       "1  fake  Buena foto Huracán Sandy, recuerda película Dí...   \n",
       "2  fake                          Scary shit #hurricane #NY   \n",
       "3  fake  My fave place world #nyc #hurricane #sandy #st...   \n",
       "4  fake         42nd #time #square #NYC #subway #hurricane   \n",
       "5  fake  Just time #halloween photo #hurricane #sandy #...   \n",
       "6  fake  Crazy #Hurricane #Sandy prayers go family frie...   \n",
       "7  fake   #sandy #newyork #hurricane #statueofliberty #USA   \n",
       "8  fake                                    #nyc #hurricane   \n",
       "9  fake  robertosalibaba god brother #sandy #hurricane ...   \n",
       "\n",
       "                                     lemmatisedTweet  \n",
       "0  ¿Se acuerdan película: “El día después mañana”...  \n",
       "1  Buena foto Huracán Sandy, recuerda película Dí...  \n",
       "2                          Scary shit #hurricane #NY  \n",
       "3  My fave place world #nyc #hurricane #sandy #st...  \n",
       "4         42nd #time #square #NYC #subway #hurricane  \n",
       "5  Just time #halloween photo #hurricane #sandy #...  \n",
       "6  Crazy #Hurricane #Sandy prayer go family frien...  \n",
       "7   #sandy #newyork #hurricane #statueofliberty #USA  \n",
       "8                                    #nyc #hurricane  \n",
       "9  robertosalibaba god brother #sandy #hurricane ...  "
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatising\n",
    "tokeniser = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "df_train['lemmatisedTweet'] = df_train['filteredTweet'].apply(lambda x: ' '.join([lemmatiser.lemmatize(w) for w in tokeniser.tokenize(x)]))\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Design and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define features and target for training and testing\n",
    "tar_train = df_train.label\n",
    "ft_train = df_train.lemmatisedTweet\n",
    "tar_test = df_test.label\n",
    "ft_test = df_test.tweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init Bag-of-Words\n",
    "count_vectoriser = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectoriser.fit_transform(ft_train)\n",
    "count_test = count_vectoriser.transform(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init N-Gram\n",
    "ngram_vectoriser = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))\n",
    "ngram_train = ngram_vectoriser.fit_transform(ft_train)\n",
    "ngram_test = ngram_vectoriser.transform(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init TF-IDF\n",
    "tfidf_vectoriser = TfidfVectorizer(stop_words='english', max_df=0.2)\n",
    "tfidf_train = tfidf_vectoriser.fit_transform(ft_train)\n",
    "tfidf_test = tfidf_vectoriser.transform(ft_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.811\n"
     ]
    }
   ],
   "source": [
    "#Bag-of-Words\n",
    "clf.fit(count_train, tar_train)\n",
    "\n",
    "pred = clf.predict(count_test)\n",
    "score = metrics.accuracy_score(tar_test, pred)\n",
    "\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.458\n"
     ]
    }
   ],
   "source": [
    "#N-Grams\n",
    "clf.fit(ngram_train, tar_train)\n",
    "\n",
    "pred = clf.predict(ngram_test)\n",
    "score = metrics.accuracy_score(tar_test, pred)\n",
    "\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.862\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "clf.fit(tfidf_train, tar_train)\n",
    "\n",
    "pred = clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(tar_test, pred)\n",
    "\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 2319 FP: 290 TN: 919 FN: 227\n",
      "f1: 0.900\n"
     ]
    }
   ],
   "source": [
    "#Calculating F1 score\n",
    "TP = 0 \n",
    "FP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "\n",
    "for true, guess in zip(tar_test, pred):\n",
    "    if(true == 'fake' and guess == 'fake'):\n",
    "        TP = TP + 1\n",
    "    if(true == 'real' and guess == 'fake'):\n",
    "        FP = FP + 1\n",
    "    if(true == 'real' and guess == 'real'):\n",
    "        TN = TN + 1\n",
    "    if(true == 'fake' and guess == 'real'):\n",
    "        FN = FN + 1\n",
    "        \n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "f1 = 2 * ((precision * recall) / (precision + recall))\n",
    "print(\"TP: %d FP: %d TN: %d FN: %d\" % (TP, FP, TN, FN))\n",
    "print(\"f1: %0.3f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
